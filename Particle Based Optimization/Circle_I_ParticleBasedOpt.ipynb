{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circle-I\n",
    "## Particle Based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello! Thank you for downloading this notebook which is part of Circle-I program that is made for HMFT-ITB from the Innovation and Workshop Division of BP HMFT-ITB 2020/2021. The topics that you can learn from this notebook are stated below.\n",
    "\n",
    "* Gradient-Free Optimization\n",
    "* Examples of Particle Based Optimization\n",
    "* Particle Swarm Optimization\n",
    "* Uses of Particle Based Optimization\n",
    "\n",
    "Many thanks to the contributors to this module and hopefully this will help in your future innovations and research!\n",
    "\n",
    "Extra links for reading:\n",
    "\n",
    "* https://medium.com/analytics-vidhya/implementing-particle-swarm-optimization-pso-algorithm-in-python-9efc2eb179a6\n",
    "* https://towardsdatascience.com/nature-inspired-optimization-algorithms-particle-swarm-optimization-2cd207d0d37e\n",
    "* https://github.com/Vermillord/projects/blob/master/KP_Notes.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-Free Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Graph Types](resources/67963471-91505b80-fbf6-11e9-8718-e9462c829368.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization acts as an important part of finding solutions for:\n",
    "\n",
    "* Non-differentiable functions\n",
    "* Multiple local minima (Multi-modal)\n",
    "* Multiple Objectives\n",
    "* Etc.\n",
    "\n",
    "The usual method that's used for optimization is a gradient-based optimizer. An algorithm that is gradient-based finds the minima point of an optimization problem by using a gradient descent method. This method is suitable for simple problems, but for a more complex problems that is noisy, discontinous, multi-modal, etc. the gradient-based method in most cases would fail in finding the global minima in the search space. To ensure that an algorithm can find the global minima of an optimization problem we can implement a gradient-free method.\n",
    "\n",
    "Gradient-free method mimics mechanisms observed in nature or use heuristics. The gradient-free method differs greatly from the gradient-based because of its independency from using gradient as a source of information. Gradient-free methods can find multiple good solutions from a search space, but sometimes the solution found isn't always the best one. Compared to the gradient-based, this method can find global minima or at least multiple good solutions from a search space that has multiple local minima. One of the most commonly used and well known algorithm is the Particle Swarm Optimization, which we will discuss in this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle-Based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particle-Based Optimization is a common Gradient-Free method that uses particles as a source of information. The most well known algorithm, which you might have heard of, is the Particle Swarm Optimization. A Particle-Based Optimization mostly consists of three parts; the particle initialization, \"moving\" the particles, and iterating until convergence. Other examples of Particle-Based Optimization are:\n",
    "\n",
    "* Genetic Algorithm\n",
    "* Ant and Bee Algorithm\n",
    "* Flower Pollination Algorithm\n",
    "* Brain Storm Optimization\n",
    "* etc.\n",
    "\n",
    "The algorithms above are all inspired by nature; Genetic Algorithm is inspired by Darwin's theory of evolution, Ant and Bee Algorithm is inspired by ants and bees as the name says, Flower Pollination Algorithm is inspired by pollination done by flowers, and the Brain Storm Optimization is inspired by the brainstorming process that humans do. Each algorithms has its own advantages and limitations, but mostly it acts the same where particles are used to find local minima point in a search space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Swarm Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of optimization using a swarming method is called as the Particle Swarm Optimization (PSO). PSO doesn't use gradient descent and it can solve both linear and non-linear problems. PSO uses particles as a source of information, precisely their position and velocity. Both information are used for updating the value that is searched. The formulation for the position update is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x^{i}_{k+1} = x^{i}_{k} + v^{i}_{k+1}\\Delta t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas for the velocity the formulation for updating its value is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v^{i}_{k+1} = wv^{i}_{k} + c_{1}r_{1}\\frac{p^{i}_{k} - x^{i}_{k}}{\\Delta t} + c_{2}r_{2}\\frac{p^{g}_{k} - x^{i}_{k}}{\\Delta t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $r_1$ and $r_2$ are both random number between [0,1]\n",
    "* $p^{i}_{k}$ is the particle's best position at interval $k$ and $p^{g}_{k}$ is the swarm's best position at interval $k$\n",
    "* $c_1$ is the confidence for the particle and $c_2$ is the confidence for the swarm\n",
    "* $w$ is the inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another fact that we can observe through the equation is by subtituting the velocity update function to the position update function. By doing so, we can obtain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x^{i}_{k+1} = x^{i}_{k} + wv^{i}_{k}\\Delta t + c_{1}r_{1}(p^{i}_{k} - x^{i}_{k}) + c_{2}r_{2}(p^{g}_{k} - x^{i}_{k})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x^{i}_{k+1} = x^{i}_{k} + wv^{i}_{k}\\Delta t + (c_{1}r_{1} + c_{2}r_{2}) \\left(\\frac{c_{1}r_{1}p^{i}_{k} + c_{2}r_{2}p^{g}_{k}}{c_{1}r_{1} + c_{2}r_{2}} - x^{i}_{k}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the equation above with the line-search function which is defined as\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_{k} + \\alpha_{k}p_{k}\n",
    "$$\n",
    "\n",
    "We can see that the three different parts of the equation represents a part of the line-search function\n",
    "\n",
    "* $x^{i}_{k} + wv^{i}_{k}\\Delta t$ equals to $x_{k+1}$\n",
    "\n",
    "* $(c_{1}r_{1} + c_{2}r_{2})$ equals to $\\alpha_{k}$\n",
    "\n",
    "* $\\left(\\frac{c_{1}r_{1}p^{i}_{k} + c_{2}r_{2}p^{g}_{k}}{c_{1}r_{1} + c_{2}r_{2}} - x^{i}_{k}\\right)$ equals to $p_{k}$\n",
    "\n",
    "Hence, we can say that each particle searches the value based on a line-search with a stochastic step size ($r_1$ and $r_2$ are random numbers) and search direction (again there are $r_1$ and $r_2$ components)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to create the PSO algorithm in Python. First, let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from numba import njit, jit, prange\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps in creating the PSO algorithm are:\n",
    "\n",
    "1. Initialize the particles by using a lower and upper boundary with a random number generator\n",
    "1. Initialize the conditions for the position and velocity arrays\n",
    "1. Set the local and global best value as high as possible\n",
    "1. Find the fitness value of a particle by using a cost function\n",
    "1. Compare the cost function value with the current local and global value\n",
    "1. Update the particle local and best position\n",
    "1. Update the particles using the position and vector update function\n",
    "1. Do steps (4) to (6)\n",
    "1. Loop from (7) until max iteration is reached\n",
    "\n",
    "To make it easier, the PSO algorithm are half-made in the cell below. As standard the variables that we will use in the algorithms are:\n",
    "\n",
    "* ppos_vector: Particle positional vector\n",
    "* pvel_vector: Particle velocity vector\n",
    "* pbest_pos: Local best position\n",
    "* pfit_value: Fitness value of a particle\n",
    "* gbest_pos: Global best position\n",
    "* gfit_value: Global best value of the swarm\n",
    "\n",
    "Based on the variable name information and the steps mentioned, try to complete the algorithm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSO(c_func, n_param, particles, lb, ub, iterate_max):\n",
    "    # Define the constants\n",
    "    w = 0.8\n",
    "    c1 = 0.7\n",
    "    c2 = 0.9\n",
    "    iterate = 0\n",
    "    \n",
    "    # Setup the initial conditions for position and velocity arrays\n",
    "    ppos_vector = np.random.uniform(lb, ub, (particles, n_param))\n",
    "    pbest_pos = ppos_vector\n",
    "    pfit_value = np.ones(particles) * 1e100\n",
    "    gbest_pos = np.zeros(n_param)\n",
    "    gfit_value = 1e100\n",
    "    pvel_vector = np.zeros((particles, n_param))\n",
    "    \n",
    "    # First loop for assigning the fitness value using the cost function\n",
    "    for i in range(particles):\n",
    "        # Check the position of individual and group value using the evaluation function\n",
    "        cost_func = c_func(ppos_vector[i])\n",
    "        \n",
    "        # Update each values using the cost functions\n",
    "        if(pfit_value[i] > cost_func):\n",
    "            '''\n",
    "            INSERT YOUR CODE HERE\n",
    "            '''\n",
    "            \n",
    "        if(gfit_value > cost_func):\n",
    "            '''\n",
    "            INSERT YOUR CODE HERE\n",
    "            '''\n",
    "        \n",
    "    # Second loop for implementing the PSO Algorithm\n",
    "    while (iterate < iterate_max):\n",
    "        for i in range(particles):\n",
    "            # Update the velocity and position vector\n",
    "            '''\n",
    "            INSERT YOUR CODE HERE BASED ON THE EQUATION MENTIONED IN THE TEXT ABOVE\n",
    "            '''\n",
    "            \n",
    "            cost_func = c_func(ppos_vector[i])\n",
    "            \n",
    "            # Update each values using the cost functions\n",
    "            if(pfit_value[i] > cost_func):\n",
    "                '''\n",
    "                INSERT YOUR CODE HERE\n",
    "                '''\n",
    "                \n",
    "            if(gfit_value > cost_func):\n",
    "                '''\n",
    "                INSERT YOUR CODE HERE\n",
    "                '''\n",
    "        \n",
    "        iterate = iterate+1\n",
    "        print(\"Iteration: \", iterate, \" | Global best cost: \", c_func(gbest_pos))\n",
    "    \n",
    "    gbest_loss = c_func(gbest_pos)\n",
    "    \n",
    "    print(\"The best position for each parameter: \", gbest_pos, \" with \", iterate, \" iteration.\")\n",
    "    return gbest_pos, gbest_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the PSO algorithm, let's load a model to optimize. Try to load the dataset1.csv file from the resources folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('resources/dataset1.csv')\n",
    "x0 = np.array(df.x[1:])\n",
    "y0 = np.array(df.y[1:])\n",
    "\n",
    "plt.title('Dataset')\n",
    "plt.xlabel('x value')\n",
    "plt.ylabel('y value')\n",
    "plt.scatter(x0, y0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the graph above, the PSO algorithm will find a global minima by using an evaluation function stated below\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta_1(1 - e^{\\beta_2 x^{\\beta_3}})\n",
    "$$\n",
    "\n",
    "and the cost value of the predicted data is\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{M} \\sum_{i=1}^M \\left(y - \\hat{y} \\right)^2\n",
    "$$\n",
    "\n",
    "where $M$ is the number of data, $\\beta_1$, $\\beta_2$, $\\beta_3$ is a constant. To make sure that the algorithm finds the correct value of the parameters, we need to define some constrains which will make the cost value outside the defined constrain as high as possible. The constrains are determined based on the shape of the graph where $\\beta_1 > 0$, $\\beta_2 < 0$, and $\\beta_3 > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(param):\n",
    "    if(param[0]<0 or param[1]>0 or param[2]<0):\n",
    "        cost = np.inf\n",
    "    err = y0 - '''INSERT YOUR CODE HERE'''\n",
    "    cost = np.mean(err**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pso_best, pso_loss = PSO(cost, 3, 25, -1, 1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print out the resulting graph based on the prediction, we need to define the evaluation function so that the predicted data can be generated accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ev_func(param):\n",
    "    return '''INSERT YOUR CODE HERE'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('PSO Predict Model 1')\n",
    "plt.xlabel('x value')\n",
    "plt.ylabel('y value')\n",
    "plt.scatter(x0, y0, label='Dataset-1', color='lightgray')\n",
    "plt.plot(x0, ev_func(pso_best), label='PSO Model-1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another dataset that is similar to the first dataset but it has another term added to the evaluation function that changes the data. First, let's load the data which is the dataset2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('resources/dataset2.csv')\n",
    "x0_ = np.array(df.x[1:])\n",
    "y0_ = np.array(df.y[1:])\n",
    "\n",
    "plt.title('Dataset')\n",
    "plt.xlabel('x value')\n",
    "plt.ylabel('y value')\n",
    "plt.scatter(x0_, y0_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the dataset it can be seen that there is an added sinusoidal term to the evaluation function (notice the waves in the plot). The evalution function now is defined as\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta_1(1 - e^{\\beta_2 x^{\\beta_3}}) + \\beta_4sin(\\beta_5 x)\n",
    "$$\n",
    "\n",
    "whereas the cost value will use the same function as before. The constrain that we will give to the new constant is $\\beta_4 > 0.01$ and $\\beta_5 > 1.0$. The consideration of the value chosen for the constrains is to make sure that the sinusoidal term of the new evaluation function affects the predicted model, because if it's too low the sinusoidal term won't give any new effect to the predicted model. Now, let's define the modified cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_cost(param):\n",
    "    if(param[0]<0 or param[1]>0 or param[2]<0 or param[3]<0.01 or param[4]<1.0):\n",
    "        return np.inf\n",
    "    err = y0_ - (param[0]*(1-np.exp(param[1]*x0_**param[2]))+param[3]*np.sin(param[4]*x0_))\n",
    "    cost = np.mean(abs(err))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pso_best_, pso_loss_ = PSO(mod_cost, 5, 50, -2, 2, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, we need to define the evaluation function first to plot the predicted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ev_func_mod(param):\n",
    "    return param[0]*(1-np.exp(param[1]*x0_**param[2]))+param[3]*np.sin(param[4]*x0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('PSO Predict Model 2')\n",
    "plt.xlabel('x value')\n",
    "plt.ylabel('y value')\n",
    "plt.scatter(x0, y0, label='Dataset-2', color='lightgray')\n",
    "plt.plot(x0, ev_func_mod(pso_best_), label='PSO Model-2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the second predicted dataset it can be seen that when the PSO is used to predict the parameters, it is unstable compared to the first dataset. Unstable here means the cost value that the PSO finds doesn't always converge onto one single global minima point. The convergence of the particles are determined by the number of parameters, complexity of the model, the particle initialization range, and the constrains. To find the best solution sometimes there needs to be a number of iteration done to ensure that the best local minima is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uses of Particle-Based Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization is a process to find the best solution by comparing solution iteratively. Particle-Based Optimization comes in as a tool to help in finding those solutions. By using algorithms we can minimize the time it takes to find a solution when we do it manually. Most science and engineering field use these algorithms to optimize models that are made. Examples of these are:\n",
    "\n",
    "* PID controller tuning\n",
    "* Model prediction\n",
    "* Route optimization\n",
    "* Physical quantity optimization\n",
    "* Image processing\n",
    "* etc.\n",
    "\n",
    "Outside science and engineering, optimization algorithms can be used in several fields. Some examples are to find the best profit from a business model, optimizing schedules, find the best price for selling items, etc. There are various other examples that you can find other than the ones that are mentioned here. Papers and journals are the best source of information for finding optimization algorithm methods, you just have to use the right keywords!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this module of Circle-I! We hope that this module can expand your knowledge about optimization algorithms and its uses. Thank you for downloading this module and stay tuned for more Circle-I!\n",
    "\n",
    "\n",
    "#KaryakanIndonesia\n",
    "#AestheticallyBarbaric"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
